

# 初始化⽅案的选择在神经⽹络学习中起着举⾜轻重的作⽤，它对保持数值稳定性⾄关重要

# 要么是梯度爆炸（gradient exploding）问题：参数更新过⼤，破坏了模型的稳定收敛；
# 要么是梯度消失（gradient vanishing）问题：参数更新过⼩，在每次更新时⼏乎不会移动，导致模型⽆法学习

# 梯度消失

#经sigmoid函数1/(1 + exp(−x))（4.1节提到过）很流⾏，因为它类似于阈值函数。由于早期的⼈⼯神经⽹
# 络受到⽣物神经⽹络的启发，神经元要么完全激活要么完全不激活（就像⽣物神经元）的想法很有吸引⼒。
# 然⽽，它却是导致梯度消失问题的⼀个常⻅的原因
import torch
from d2l import torch as d2l
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.sigmoid(x)
y.backward(torch.ones_like(x))
d2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()], legend=['sigmoid', ' gradient'], figsize=(4.5, 2.5))

# 当sigmoid函数的输⼊很⼤或是很⼩时，它的梯度都会消失。此外，当反向传播通过许多
# 层时，除⾮我们在刚刚好的地⽅，这些地⽅sigmoid函数的输⼊接近于零，否则整个乘积的梯度可能会消失
###########################################################################
# 更稳定的ReLU系列函数已经成为从业者的默认选择
#####################################################################

# 梯度爆炸
# 我们⽣成100个⾼斯随机矩阵，并将它们与某个初始矩阵相乘。对于我们选择的尺度（⽅差σ2 = 1），矩阵乘积发⽣爆炸
M = torch.normal(0, 1, size=(4, 4))
print('一个矩阵\n', M)
for i in range(100):
    M = torch.mm(M, torch.normal(0,1,size=(4,4)))

print('乘以100个矩阵后\n', M)

# 参数初始化
# 使⽤正态分布来初始化权重值。如果我们不指定初始化⽅法，框架将使⽤默认的随机初始化⽅法，对于中等难度的问题，这种⽅法通常很有效。

# 这就是现在标准且实⽤的Xavier初始化的基础，它以其提出者 [Glorot & Bengio, 2010] 第⼀作者的名字命名。
# 通常，Xavier初始化从均值为零，⽅差 σ2 =2 / nin+nout 的⾼斯分布中采样权重。